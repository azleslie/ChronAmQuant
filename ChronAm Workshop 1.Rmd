---
title: "Text Mining Approaches with Historical Newspapers, Part 1"
author: "[Alex Leslie](https://english.rutgers.edu/cb-profile/ahl80.html)"
date: "October 30, 2019"
output: 
  pdf_document:
    toc: true
urlcolor: blue
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, tidy=TRUE)
if (!(require(dplyr))) {
  install.packages("dplyr", repos = "http://cran.us.r-project.org")
}
if (!(require(stringr))) {
  install.packages("stringr", repos = "http://cran.us.r-project.org")
}
if (!(require(knitr))) {
  install.packages("knitr", repos = "http://cran.us.r-project.org")
}
library("dplyr")
library("stringr")
library("parallel")
library("knitr")
```

Welcome! In this workshop we'll be exploring one technique and one kind of dataset: fuzzy string matching in R and a batch of digitized newspapers from Chronicling America.^[My thanks to Nicole Sheriko and Andrew Goldstone for their feedback and advice on portions of the code included here.]

The first step is to download the newspaper batch we'll be using [from Chronicling America](https://chroniclingamerica.loc.gov/ocr/). The batch we'll be working with is called `allspice`; it consists of the *Perth Amboy Evening News* from March 1903 to March 1907. Once downloaded, unzip the tarball; this will take a few more minutes. The result will show up as a single directory with the name "sn85035720": this is Chronicling America's ID for the *Perth Amboy Evening News*, so we'll leave it that way. Any other single-paper Chronicling America batch will do just as well if you'd prefer to work with data from a different newspaper. Just make sure that [1] the batch includes enough page data to produce a good number of search results (`allspice` includes nearly 9,000 pages) and [2] you change all instances of "sn85035720" in this code to the ID of whatever other paper you choose.

Before we get any further into the news, though, we should familiarize ourselves with fuzzy string matching.

#Levenshtein Distances and Fuzzy String Matching
The first obstacle to doing quantitative analysis on digitized newspapers - or even just finding a search term - is the messiness of newspaper Optical Character Recognition (OCR). This is a problem for all periodicals, but it's an especially big one for newspapers, many of which were printed on low-quality paper with small fonts and received minimal preservation attention. If we're going to make use of this wealth of data, we need to be able to work around persistent, inconsistent OCR errors. 

Say we're trying to find a name, the surname of postbellum American author Mary Murfree, from among a *vector* of *strings* (a series of words).

```{r, tidy=TRUE}
test_vector <- c("murfree", "muurfree", "nurfree", "murfre", "murrfee", "murphee", 
                 "durpree", "free", "smurffree", "murfreesboro", "marymurfree")
```

At least one of these words is perfectly correct ("murfree") and some of them are obviously not what we're looking for ("durpree"), but some of them are quite probably just OCR errors ("nurfree"). R has a conventional search function for exact *character patterns* (for our purposes, words): `grep`.

```{r}
grep("murfree", test_vector)
```

`grep` returns a vector of the *numeric position* of each *element* in the specified vector in which our desired character pattern ("murfree") occurs. To return a vector of the strings themselves - the elements of `test_vector` and not just their numeric position - we'll save our hits positions as the vector `search_hits` and then *index* that vector *into* our `test_vector` of strings, with brackets. 

```{r}
search_hits <- grep("murfree", test_vector)
test_vector[search_hits]
``` 

Unsurprisingly, `grep` doesn't do us much good here. Note that it also includes strings longer than our search pattern. This is basically the result we'd get if we searched for a particular word via the Chronicling America's search API. Thankfully, R has something much better for us: `agrep`.

```{r}
search_hits <- agrep("murfree", test_vector, max.distance=2)
test_vector[search_hits]
```

`agrep` allows us to get fuzzy; it returns all elements of a vector in which an approximate match to our character pattern occurs, within a specified Levenshtein Distance (`max.distance=`). Levenshtein distance is simply the number of insertions (internal added characters), deletions (deleted characters), and substitutions (characters replaced by other characters) it takes to get from one string to another (Table 1).

```{r levenshtein table, echo=FALSE}
Edit <- c("Insertion", "Deletion", "Substitution")
String <- c("muurfree", "murfre", "nurfree")
Difference <- c("+u", "-e", "m->n")
Distance <- c("1", "1", "1")
frame <- data.frame(String, Difference, Distance, Edit)
kable(frame, caption="A few Levenshtein Distances")
```

A Levenshtein Distance of two is clearly a bit too capacious for this test dataset, though, so let's try again:

```{r}
search_hits <- agrep("murfree", test_vector, max.distance=1)
test_vector[search_hits]
```

By cutting the `max.distance` down to one, we're able to filter out a few more things we didn't want ("murphee" and "durpree") - but notice we also lost one string we probably *did* want to keep, "murrfee". Our goal in setting a `max.distance` is to minimize the number of false negatives (the strings we do want that our search fails to identify) *and* false positives (the search hits that we don't actually want).

We're still getting some garbage though, for the same reason `grep` gave us additional hits: `agrep` also includes strings longer than our search pattern so long as something within distance of that pattern occurs within the string. We can solve this issue by taking the results of our last `agrep` call and filtering out anything more than one character longer than "murfree":

```{r}
search_hits <- which(nchar(test_vector[search_hits]) < nchar("murfree")+2)
test_vector[search_hits]
```

By adding a length limit to our search hits, we can finally get rid of some more pesky strings we didn't want. Once again, though, this comes at a cost: we've lost "marymurfree," which is certainly a false negative. If we wanted to be even more precise, we could address this. We might run an additional `grep` call that looks specifically for the pattern "marymurfree" and combine its results with the results of our `agrep` call. Perhaps we're worried that the OCR software has a tendency to split "murfree" into separate words, like "mur" and "free"; we could write some additional code to `paste` each string together with each string immediately adjacent to it before calling `grep` and then combine those results with the results of our `agrep` call.

Our code will run slower, however, the more intricate our search gets. On a small dataset or even a large dataset and a lot of time, this might not be a problem. But even working with a few years of a single paper is a sizeable chunk of data: 9,632 pages containing around 38 million words that can take up over 540 Mb of memory. Furthermore, no matter how precise we get, at the end of the day fuzzy string matching is always a gamble: with enough data, we will always have false positives and false negatives.

#Data Familiarity and Setting Up an Input Vector
Now let's turn to the *Perth Amboy Evening News*. Chronicling America datasets come highly structured; open up your downloads folder and familiarize yourself with it for a bit. You'll notice the file directory is structured something like this: "E:/Alexander/Downloads/[paper ID]/[year]/[month]/[day]/[edition]/[page]". Once you get all the way to the bottom of things, you'll see two files: a plain old .txt file and an .xml file that indicates the position of the text on the page.

The .txt file is the only one we want - and in fact, we *do not* want the .xml files, which take up tons of space. This next line of code uses wildcards to remove all files with the .xml extension within the exact number of directories specified by asterisk. Change the names or number of the directories prior to "sn85035720" to match the file path on your computer (depending on your archive utility, you may have an additional directory in there by the name of "njr_allspace_ver02"). But **do not** remove "sn85035720" (unless replacing it with the ID for a different newspaper you downloaded instead) or change anything after it: we don't want to go removing other things from your computer. If you're not sure what the exact file path should look like, just find the "sn85035720" in your file viewer, right-click (Ctrl+click on Mac), and select "Properties" to view its location.

```{r}
unlink(Sys.glob(file.path("E:/Alexander/Downloads/sn85035720/*/*/*/*/*", "*.xml")))
```

A brief note: downloading isn't the only way to access this data. Since Chronicling America hosts a plain text version of each page online as well, we could scrape those webpages instead. If you're interested in this approach, you might consult my [workshop on webscraping techniques](https://github.com/azleslie/WebscrapeTechniques).^[While scraping is a good way to obtain your own copy of the data or even to run moderate/test searches, it is much more efficient - and much kinder to Chronicling America's servers - to run code on copies of files rather than scraping each page for every query.]

In order to run our search, we need to figure out the file paths for each file of data. R has a convenient function for this: `list.dirs`. `list.dirs` returns every directory in a particular path. We can narrow things down, however, and only return the directories that are x degrees removed in our file tree. We don't even need to know exact names, because `list.dirs` supports wildcard matching with `*` to designate directories. An illustration should clarify:

```{r}
filepaths <- list.dirs(Sys.glob("E:/Alexander/Downloads/sn85035720/*/*/*/*/*"))
filepaths[1:12]
```

This vector is just about all the input R needs to read in a single page of data. Let's test it by reading in a sample page with `readLines`. Rather than specifying the exact name of the file (which we could just as easily do in this case, since we know that they're all called "ocr.txt"), we'll grab any .txt file(s) in the directory with the combination of the `Sys.glob` and `file.path` functions.

```{r, warning=FALSE}
test_page <- readLines(Sys.glob(file.path(filepaths[1], "*.txt")))
```

The metadata pertaining to each of the page .txt files isn't contained in the file names or even in the files themselves: we need to extract each piece of information from each file path. This can be done with the `strsplit` function by splitting wherever there is a `/` character.

```{r}
strsplit(filepaths[1:3], "/")
```

Note that the result is a series of vectors, one corresponding to each element of the filepaths vector, rather than one big vector with everything mashed together. This data format is called a *list*. When indexing a list, we use a double pair of brackets to index the vector and an additional single pair of brackets to index the element within that vector. Since the year is the fifth element of each vector, then, we would index the year of the first file as such:

```{r}
strsplit(filepaths[1], "/")[[1]][5]
```

Great: now we know how to get everything we want out of a single input vector. This will make our code efficient and clean. 

There's one other useful thing to be done with file paths, and that is identifying the number of pages in each issue. Since each of our filepaths is the directory corresponding to a single page, we'll need to back up one directory. We can do this by manipulating each element of filepaths with `gsub`, to substitute everything (`(^.*)`) from the beginning of each file path to the part that reads `/seq-` followed by a digit (`\\d`) at the end (`$`). These are metacharacters that, in programming, are used to build *regular expressions*: expressions that refer to inexact yet consistent character patterns. No matter which element of filepaths we use this line of code on, it'll still work because they all follow the same pattern.

```{r}
gsub("(^.*)/seq-\\d+$", "\\1", filepaths[1])
```

Now, we just want to know how many files are in this directory. To do so we'll use `Sys.glob` again in conjunction with `list.files`. We don't really want to know the names of the files, though, just how many of them there are. This is what the `length` function does.

```{r}
length(list.files(Sys.glob(gsub("(^.*)/seq-\\d+$", "\\1", filepaths[1]))))
```

If we were working with multiple newspapers, we would simply change two things. First - and I do mean first - we would put all the newspaper directories into one big directory to make sure we didn't mess things up with our wildcards. Second, we would add the name of that big directory to our file path above and replace the "sn85035720" above with another `*`. (The indexed positions in the function below would need to change as well to reflect whatever additional changes were made.) But aside from that, the code would essentially remain the same.

#Running a Search and Grabbing Collocates
Now it's time to start putting everything together. So far we've been using functions included in R. Now we'll write one of our own, `searchfun`, that takes an element of our input vector, reads in a page, finds all approximate matches using the `agrep` code we wrote two sections ago, and records the metadata for each result using the code from the previous section. 

Our code needs to be written as a function because this way we can easily run it multiple times, i.e., for each page. But it does one other thing as well: it also returns the twenty strings on either side of each hit, or collocate strings. This takes little extra time and will come in very handy later; indeed, it's one of the biggest advantages to using our own code rather than relying on a search API. In this example, I've used postbellum American author Jack London as my search.

```{r, tidy=TRUE, warning=FALSE}
searchfun <- function (filepath) { 
  # the name of our function and the variable it requires to run
  
  one_page <- readLines(Sys.glob(file.path(filepath, "*.txt")))
  # read any .txt file(s) in the filepath into memory as `one_page`
  
  one_page <- unlist(strsplit(one_page, "\\W+"))
  one_page <- tolower(one_page[one_page != ""]) 
  # split the page into words and make everything lowercase
  
  forename <- agrep("jack", one_page, max.distance=1)
  forename <- forename[which(nchar(one_page[forename]) < nchar("jack")+2)]
  # here are the two lines for fuzzy string matching from earlier
  
  surname <- agrep("london", one_page[forename+1], max.distance=1)
  surname <- surname[which(nchar(one_page[forename[surname]+1]) < nchar("london")+2)]
  # these lines are basically the same, except they only check the strings immediately 
  # following the hits identified in our previous search, `forename`
  
  collocates <- lapply(surname, 
                    function(x) if (forename[x] < 21) {
                       paste(one_page[1:(forename[x]+22)], collapse=" ")
                    } else {
                       paste(one_page[(forename[x]-20):(forename[x]+22)], collapse=" ")
                    })
  # this is where we grab collocate strings (for ease of storage, we'll paste them all 
  # into one string for now); we're indexing with `surname` because we only want to keep 
  # track of instances of forename immediately followed by surname. we use `if` in order
  # to avoid getting a negative subscript (i.e., when the desired string appears at the
  # start of a page)
  
  tot_pg <- length(list.files(Sys.glob(gsub("(^.*)/seq-\\d+$", "\\1", filepath))))
  # this finds the total number of page files for the issue; it'll be useful later
  
  rm(one_page, forename, surname)  
  gc()
  # this clears our page data out of memory now that we're done with it
  
  if (length(collocates)==0) {
    return()   
    # if there are no hits, nothing will happen
    
    } else {
    return(data.frame(LCCN="sn85035720",
                      Year=strsplit(filepath, "/")[[1]][5], 
                      Month=strsplit(filepath, "/")[[1]][6], 
                      Day=strsplit(filepath, "/")[[1]][7],
                      Page=strsplit(filepath, "/")[[1]][9], 
                      Issue_Length=tot_pg, Collocates=I(collocates)))}
    # if there are hits, we return the date, page, issue length, and collocates for each
}
```

Now it's time to decide who or what two-word sequence to search for. To help decide, [bring up Chronicling America](https://chroniclingamerica.loc.gov/) before running your search. In the Advanced Search tab, select New Jersey under the State field and run a quick Phrase Search for the name or phrase you're considering to see how many hits you get. Ideally, in order to balance the amount of time this will take with the amount of data it'll produce, you should aim for someone or something with around 100 hits on Chronicling America.

Once you've done that, change both instances of "jack" and "london" to the two names or words you've decided on and run the batch of code above.

In order to make sure things don't take too long, we'll wrap our search function in another function that distributes the workload across multiple processor cores.

```{r, warning=FALSE}
decade_par <- function (input_vector) {
  core_num <- detectCores()-2
  clust <- makeCluster(core_num, outfile="")
  clusterExport(clust, varlist=c("input_vector"), envir=environment())
  clusterExport(clust, varlist=c("searchfun"))
  result <- do.call(rbind, parLapply(clust, seq_along(input_vector),
                              function(x) searchfun(input_vector[x])))
  # this is where the magic happens: it runs `searchfun` once for each element of 
  # `input_vector` and then binds the results together
  
  stopCluster(clust)
  return(result)
}
```

Finally, let's run the actual search! This will take a couple minutes.

```{r}
hits <- decade_par(filepaths)
```

Now if all has gone well, the output of this function, `hits`, will be a data frame in which each row contains the metadata and collocate string for each hit:

```{r}
hits
```

Nice and tidy data: this will make further analysis a breeze. Our work has paid off!

You may be wondering: how useful was fuzzy string matching after all? We can see how many hits an exact search would've missed by doing an exact pattern search (with `grep`) to see how many of our collocate strings contain it and subtracting this number from the total number of hits:

```{r}
nrow(hits) - length(grep("jack london", hits$Collocates))
```

And now the follow-up question: are these extra hits actually matches or are they false positives? Let's take a look at their collocate strings. To do so, we'll use the `-` sign for negative indexing, to return all collocate strings that don't contain the exact character pattern "jack london"; then we'll split the big collocates string (with `str_split`) into separate strings wherever there's a space (`" "`).

```{r}
str_split(hits$Collocates[-grep("jack london", hits$Collocates)], " ")
```

These look pretty good: four are explicitly about London's fiction (1, 3, 4, and 5), two are about London's own travel plans (6 and 7), and one particularly fun result is a poem about wanting London to write more (8) (Figure 1). Only one of these (2) is a false positive: it's a snippet called "London Tit-Bits" that just happens to be preceded by the word "back."

Seven out of eight is pretty good, especially when the collocates are as clean as they are here: this additional word data will enrich any future analysis. Our high success rate of recovering false negatives might lead us to try a slightly fuzzier search. We could, for example, change the `agrep` `max.distance` to 2 for "london"; this might open the garbage floodgates, but it could recover enough additional false negatives to be worthwhile. Fuzzy string matching is always a matter of trial and error.

![From the *Perth Amboy Evening News*, 11-17-1906, pg. 4](Page Images/london_poem.png)

The utility of fuzzy string matching can vary considerably based on [1] the distinctiveness of the desired character pattern, and [2] the messiness of the OCR data. The OCR for the *Perth Amboy Evening News* is comparatively clean for a newspaper - thanks to a lot of great work by Caryn Radick and the rest of the [New Jersey Digital Newspaper Project](https://blogs.libraries.rutgers.edu/njdnp/) team! - though still not as clean as, say, hathitrust.org scans of a magazine volume. It may be that fuzzy string matching isn't all that useful in a particular case, but you'll find that the payoff always increases as you increase the size of the newspaper dataset.

The `write.csv` function will save the results to a .csv (a bare-bones spreadsheet) for fast and easy access in the future: change the name (but not the ".csv" extension) as desired. If you're planning on coming to Part 2 of this workshop, save this file to a flashdrive or cloud drive and you'll be able to pick up right where you left off.

```{r}
write.csv(hits, "name-this-file.csv")
```

If youâ€™d like to look at this workshop in more detail or run the code on your own, visit https://github.com/azleslie/ChronAmQuant.

Finally, we would really appreciate it if you took a minute to [fill out our brief feedback survey](https://rutgers.ca1.qualtrics.com/jfe/form/SV_a3itiZN18dY3fc9).

Thanks for participating!
